from src.data_ingestion.hn_scraper import fetch_hn_ai_stories
from src.data_ingestion.rss_parser import fetch_official_rss
from src.data_ingestion.hf_papers import fetch_hf_daily_papers
from src.filtering.dedup_engine import ArticleFilter


if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦æ¼æ–—å¼éæ¿¾èˆ‡å»é‡å±¤\n")

    print("ğŸ“¡ éšæ®µä¸€ï¼šæ•¸æ“šæ¡é›†ä¸­ ...")
    all_articles = []

    for name, fetcher, kwargs in [
        ("Hacker News", fetch_hn_ai_stories, {"hours": 24}),
        ("RSS Feeds", fetch_official_rss, {"hours": 24}),
        ("HF Papers", fetch_hf_daily_papers, {}),
    ]:
        try:
            results = fetcher(**kwargs)
            all_articles.extend(results)
            print(f"  âœ… {name}: {len(results)} ç¯‡")
        except Exception as e:
            print(f"  âŒ {name} å¤±æ•—: {e}")

    print(f"\n{'='*60}")
    print(f"  éæ¿¾å‰ï¼šå…± {len(all_articles)} ç¯‡æ–‡ç« ")
    print(f"{'='*60}\n")

    print("ğŸ”¬ éšæ®µäºŒï¼šèªæ„å»é‡ä¸­ ...")
    engine = ArticleFilter()
    unique_articles = engine.process(all_articles)

    print(f"\n{'='*60}")
    print(f"  éæ¿¾å¾Œï¼šå…± {len(unique_articles)} ç¯‡ç¨ç«‹äº‹ä»¶")
    print(f"{'='*60}\n")

    print("ğŸ“‹ å‰ 3 ç¯‡å»é‡çµæœé è¦½ï¼š\n")
    for i, article in enumerate(unique_articles[:3]):
        print(f"  [{i+1}] {article.title}")
        print(f"      ä¾†æº: {article.source}")
        if article.similar_sources:
            print(f"      ğŸ”— å·²åˆä½µç›¸ä¼¼ä¾†æº: {', '.join(article.similar_sources)}")
        else:
            print(f"      ğŸ”— ç„¡ç›¸ä¼¼æ–‡ç« è¢«åˆä½µ")
        print()

    print(f"{'='*60}")
    print("ğŸ éæ¿¾å±¤æ¸¬è©¦å®Œç•¢")
    print(f"{'='*60}")
