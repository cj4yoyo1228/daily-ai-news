from src.data_ingestion.hn_scraper import fetch_hn_ai_stories
from src.data_ingestion.rss_parser import fetch_official_rss
from src.data_ingestion.hf_papers import fetch_hf_daily_papers


def test_source(name: str, fetcher, **kwargs):
    print(f"\n{'='*60}")
    print(f"  æ¸¬è©¦è³‡æ–™æºï¼š{name}")
    print(f"{'='*60}")

    try:
        articles = fetcher(**kwargs)
        print(f"âœ… æˆåŠŸæŠ“å– {len(articles)} ç­†è³‡æ–™\n")

        for i, article in enumerate(articles[:2]):
            print(f"  [{i+1}] æ¨™é¡Œ:     {article.title}")
            print(f"      ä¾†æº:     {article.source}")
            print(f"      ç™¼å¸ƒæ™‚é–“: {article.published_at}")
            print(f"      é€£çµ:     {article.url[:80]}")
            print()

        if not articles:
            print("  âš ï¸ æ­¤è³‡æ–™æºåœ¨æ™‚é–“çª—å£å…§æ²’æœ‰æŠ“åˆ°ä»»ä½•æ–‡ç« ")

    except Exception as e:
        print(f"âŒ æŠ“å–å¤±æ•—ï¼š{e}")


if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦æ•¸æ“šæ¡é›†å±¤ (Data Ingestion)")

    test_source("Hacker News (AI ç¯©é¸)", fetch_hn_ai_stories, hours=24)
    test_source("å®˜æ–¹ RSS Feeds", fetch_official_rss, hours=24)
    test_source("Hugging Face Daily Papers", fetch_hf_daily_papers)

    print(f"\n{'='*60}")
    print("ğŸ æ‰€æœ‰æ¸¬è©¦åŸ·è¡Œå®Œç•¢")
    print(f"{'='*60}")
