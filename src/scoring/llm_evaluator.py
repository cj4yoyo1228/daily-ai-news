import json
from typing import List

import openai

from src.models.schemas import RawArticle, EvaluationResult, ScoredArticle

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä½çŸ½è°·é ‚å°–å‰µæŠ• (VC) å…¼è¯çˆ¾è¡—ç§‘æŠ€ç”¢æ¥­åˆ†æžå¸«ã€‚"
    "ä½ çš„ä»»å‹™æ˜¯åš´æ ¼è©•ä¼°å‚³å…¥çš„æ–°èž/äº‹ä»¶æ–‡æœ¬ï¼Œå°‹æ‰¾å…·æœ‰å¼·å¤§å•†æ¥­è®Šç¾æ½›åŠ›ã€èƒ½æ”¹è®Šå¸‚å ´æ ¼å±€çš„ Alpha è¨Šè™Ÿã€‚\n\n"
    "è©•åˆ†ç¶­åº¦ (ç¸½åˆ† 100)ï¼š\n"
    "1. å•†æ¥­å½±éŸ¿åŠ›èˆ‡è³‡é‡‘å‹•èƒ½ (Commercial Impact, 0-40åˆ†)ï¼šç²å·¨é¡èžè³‡ã€å¤§å» é‡ç£…ä½µè³¼ã€"
    "æˆ–èƒ½æ›¿ä¼æ¥­å¤§å¹…é™æœ¬å¢žæ•ˆå¾—é«˜åˆ†ï¼›å¸¸è¦è»Ÿé«”æ›´æ–°ã€ç„¡å•†æ¥­æ¨¡å¼çš„çŽ©å…·å°ˆæ¡ˆå¾—ä½Žåˆ†ã€‚\n"
    "2. å¸‚å ´å…·é«”åº¦ (Market Specificity, 0-35åˆ†)ï¼šæœ‰æ˜Žç¢ºçš„ç›®æ¨™å®¢ç¾¤ã€å…·é«”çš„è½åœ°ä½¿ç”¨å ´æ™¯ (Use Cases)ã€"
    "æˆ–ç”¢å“å®šåƒ¹ç­–ç•¥å¾—é«˜åˆ†ï¼›è‹¥æ˜¯ç´”å­¸è¡“ç†è«–ã€æ¯«ç„¡è½åœ°å ´æ™¯çš„ç©ºæ³›é¡˜æ™¯å‰‡å¾—ä½Žåˆ†ã€‚"
    "(è¨»ï¼šæ—©æœŸé«˜æ½›åŠ›ç”¢å“å³ä½¿å°šç„¡ç‡Ÿæ”¶æˆ–èžè³‡æ•¸æ“šï¼Œåªè¦æ‡‰ç”¨å ´æ™¯æ¥µåº¦æ˜Žç¢ºä¸”å…·ç ´å£žæ€§ï¼Œäº¦å¯çµ¦äºˆé«˜åˆ†)\n"
    "3. ç”¢å“æ–°é®®åº¦èˆ‡è­·åŸŽæ²³ (Product Novelty, 0-25åˆ†)ï¼šé–‹å‰µå…¨æ–°å•†æ¥­æ¨¡å¼ã€è§£æ±ºç—›é»žçš„æ–°ç”¢å“ç™¼å¸ƒ "
    "(å¦‚ Product Hunt ç†±é–€)ã€å¤§å» çš„çªè¥²å¼æˆ°ç•¥å¾—é«˜åˆ†ã€‚\n\n"
    "å¦‚æžœç¸½åˆ† >= 65ï¼Œis_qualified å¿…é ˆè¨­ç‚º trueã€‚\n"
    "å¦‚æžœç¸½åˆ† < 65ï¼Œis_qualified è¨­ç‚º falseï¼Œexecutive_summary è¨­ç‚º nullã€‚\n\n"
    "ã€åš´æ ¼æ ¼å¼è¦æ±‚ã€‘ï¼š\n"
    "ä½ çš„è¼¸å‡ºèªžè¨€å¿…é ˆçµ•å°ä½¿ç”¨ã€Œç¹é«”ä¸­æ–‡ (zh-TW)ã€ã€‚ç¦æ­¢ç›´æŽ¥ç…§æŠ„è‹±æ–‡æ‘˜è¦æˆ–ä½¿ç”¨ç”Ÿç¡¬å­¸è¡“åè©žã€‚\n"
    "è«‹åœ¨ executive_summary æ¬„ä½ä¸­ï¼Œåš´æ ¼æŒ‰ç…§ä»¥ä¸‹çµæ§‹è¼¸å‡ºï¼ˆè«‹ä½¿ç”¨ \\n ä¾†æ›è¡ŒæŽ’ç‰ˆï¼‰ï¼š\n\n"
    "ðŸŽ¯ ç™½è©±è§£è®€ï¼š(ç”¨ä¸€å¥è©±å‘éžæŠ€è¡“èƒŒæ™¯çš„ CEO è§£é‡‹é€™å€‹ç”¢å“/äº‹ä»¶åˆ°åº•åœ¨å¹¹å˜›ï¼Œè§£æ±ºäº†ä»€éº¼ç—›é»ž)\n"
    "ðŸ’° å•†æ¥­è¡æ“Šï¼š(å…·é«”æŒ‡å‡ºé€™æœƒå¨è„…åˆ°èª°çš„ç”Ÿæ„ï¼Ÿæ›¿èª°çœä¸‹æˆæœ¬ï¼Ÿæˆ–æ˜¯å±•ç¤ºäº†ä»€éº¼æ–°çš„å•†æ¥­è®Šç¾æ¨¡å¼ï¼Ÿ)\n\n"
    "ä½ å¿…é ˆåš´æ ¼ä»¥ä¸‹åˆ— JSON æ ¼å¼å›žè¦†ï¼Œä¸è¦åŠ ä»»ä½•å¤šé¤˜æ–‡å­—ï¼š\n"
    '{"reasoning": "...", "impact_score": 0, "specificity_score": 0, '
    '"novelty_score": 0, "total_score": 0, "is_qualified": false, '
    '"executive_summary": null}'
)


def evaluate_events(articles: List[RawArticle]) -> List[ScoredArticle]:
    """ç”¨ LLM å°æ¯ç¯‡æ–‡ç« é€²è¡Œé‡åŒ–è©•åˆ†ï¼Œå›žå‚³é”æ¨™ä¸”æŽ’åºå¾Œçš„çµæžœã€‚"""
    client = openai.OpenAI()
    total = len(articles)
    scored: List[ScoredArticle] = []

    for i, article in enumerate(articles, 1):
        print(f"[Scoring] æ­£åœ¨è©•åˆ† {i}/{total} â€” {article.title[:50]}...")

        user_payload = json.dumps(
            {
                "title": article.title,
                "source": article.source,
                "content_snippet": article.content_snippet,
            },
            ensure_ascii=False,
        )

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_payload},
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
            )

            raw_json = response.choices[0].message.content
            evaluation = EvaluationResult.model_validate_json(raw_json)

            scored.append(
                ScoredArticle(article=article, evaluation=evaluation)
            )
            tag = "âœ… é”æ¨™" if evaluation.is_qualified else "â€”"
            print(f"  -> ç¸½åˆ† {evaluation.total_score} {tag}")

        except Exception as e:
            print(f"  âŒ è©•åˆ†å¤±æ•—: {e}")
            continue

    scored.sort(key=lambda s: s.evaluation.total_score, reverse=True)
    qualified_count = sum(1 for s in scored if s.evaluation.is_qualified)

    print(f"\n[Scoring] è©•åˆ†å®Œæˆï¼š{len(scored)} ç¯‡å·²è©• / {qualified_count} ç¯‡é”æ¨™ (>=65)")
    return scored
