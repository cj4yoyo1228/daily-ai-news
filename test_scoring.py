from dotenv import load_dotenv
load_dotenv()

from src.data_ingestion.hn_scraper import fetch_hn_ai_stories
from src.data_ingestion.rss_parser import fetch_official_rss
from src.filtering.dedup_engine import ArticleFilter
from src.scoring.llm_evaluator import evaluate_events


if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦ LLM é‡åŒ–è©•åˆ†çŸ©é™£\n")

    print("ğŸ“¡ éšæ®µä¸€ï¼šæ•¸æ“šæ¡é›† (HN + RSS only) ...")
    all_articles = []

    for name, fetcher, kwargs in [
        ("Hacker News", fetch_hn_ai_stories, {"hours": 24}),
        ("RSS Feeds", fetch_official_rss, {"hours": 24}),
    ]:
        try:
            results = fetcher(**kwargs)
            all_articles.extend(results)
            print(f"  âœ… {name}: {len(results)} ç¯‡")
        except Exception as e:
            print(f"  âŒ {name} å¤±æ•—: {e}")

    print(f"\nğŸ”¬ éšæ®µäºŒï¼šèªæ„å»é‡ ...")
    engine = ArticleFilter()
    unique_articles = engine.process(all_articles)

    print(f"\nğŸ§  éšæ®µä¸‰ï¼šLLM è©•åˆ† ({len(unique_articles)} ç¯‡é€å…¥) ...\n")
    qualified = evaluate_events(unique_articles)

    print(f"\n{'='*60}")
    print(f"  æ‰“åˆ†å®Œç•¢ï¼åˆæ ¼æ–‡ç« å…± {len(qualified)} ç¯‡")
    print(f"{'='*60}\n")

    for i, scored in enumerate(qualified, 1):
        ev = scored.evaluation
        print(f"  [{i}] {scored.article.title}")
        print(f"      ç¸½åˆ†: {ev.total_score} (å½±éŸ¿åŠ›:{ev.impact_score} å…·é«”:{ev.specificity_score} æ–°é®®:{ev.novelty_score})")
        print(f"      ğŸ“Œ {ev.executive_summary}")
        if scored.article.similar_sources:
            print(f"      ğŸ”— åˆä½µä¾†æº: {', '.join(scored.article.similar_sources)}")
        print()

    print(f"{'='*60}")
    print("ğŸ è©•åˆ†å±¤æ¸¬è©¦å®Œç•¢")
    print(f"{'='*60}")
